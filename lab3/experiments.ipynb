{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3612jvsc74a57bd02762fd0ccae7a6827a0de0868563b3d499c815e35640ddddc3d2dc7e9a34dcb9",
   "display_name": "Python 3.6.12 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "import utils as ut\n",
    "from tqdm import tqdm\n",
    "import mlp as mlp"
   ]
  },
  {
   "source": [
    "# Exercise 1: Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X, mean, std):\n",
    "    X -= np.outer(mean_X, np.ones(X.shape[1]))\n",
    "    X /= np.outer(std_X, np.ones(X.shape[1]))\n",
    "    return X\n",
    "    \n",
    "importlib.reload(ut)\n",
    "X_train, y_train, Y_train = ut.loadData('data_batch_1', clipping=True)\n",
    "X_val, y_val, Y_val = ut.loadData('data_batch_2', clipping=True)\n",
    "X_test, y_test, Y_test = ut.loadData('test_batch', clipping=True)\n",
    "\n",
    "## normalize with mean and std of train set \n",
    "mean_X = np.mean(X_train, axis=1)\n",
    "std_X = np.std(X_train, axis=1)\n",
    "\n",
    "X_train = normalize_data(X_train, mean_X, std_X)\n",
    "X_val = normalize_data(X_val, mean_X, std_X)\n",
    "X_test = normalize_data(X_test, mean_X, std_X)\n",
    "data = {\"X_train\": X_train, \"y_train\": y_train, \"Y_train\": Y_train, \"X_val\":X_val, \"y_val\": y_val, \"Y_val\": Y_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ut)\n",
    "X_train_whole, y_train_whole, Y_train_whole = ut.loadData('data_batch_1', clipping=True)\n",
    "\n",
    "for i in range(2,6):\n",
    "    X, y, Y = ut.loadData('data_batch_'+str(i), clipping=True)\n",
    "    X_train_whole = np.concatenate((X, X_train_whole), axis=1)\n",
    "    y_train_whole = np.concatenate((y, y_train_whole))\n",
    "    Y_train_whole = np.concatenate((Y, Y_train_whole), axis=1)\n",
    "\n",
    "n_val = 5000\n",
    "\n",
    "X_val_small, y_val_small, Y_val_small = X_train_whole[:,-n_val:], y_train_whole[-n_val:], Y_train_whole[:,-n_val:]\n",
    "X_train_whole, y_train_whole, Y_train_whole = X_train_whole[:,:-n_val], y_train_whole[:-n_val], Y_train_whole[:,:-n_val]\n",
    "\n",
    "## normalize with mean and std of train set \n",
    "mean = np.mean(X_train_whole, axis=1)\n",
    "std = np.std(X_train_whole, axis=1)\n",
    "\n",
    "X_train_whole = normalize_data(X_train_whole, mean, std)\n",
    "X_val_small = normalize_data(X_val_small, mean, std)\n",
    "\n",
    "data_whole = {\"X_train\": X_train_whole, \"y_train\": y_train_whole, \"Y_train\": Y_train_whole, \"X_val\":X_val_small, \"y_val\": y_val_small, \"Y_val\": Y_val_small}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ut)\n",
    "X_train_whole, y_train_whole, Y_train_whole = ut.loadData('data_batch_1', clipping=True)\n",
    "\n",
    "for i in range(2,6):\n",
    "    X, y, Y = ut.loadData('data_batch_'+str(i), clipping=True)\n",
    "    X_train_whole = np.concatenate((X, X_train_whole), axis=1)\n",
    "    y_train_whole = np.concatenate((y, y_train_whole))\n",
    "    Y_train_whole = np.concatenate((Y, Y_train_whole), axis=1)\n",
    "\n",
    "n_val = 1000\n",
    "\n",
    "X_val_small, y_val_small, Y_val_small = X_train_whole[:,-n_val:], y_train_whole[-n_val:], Y_train_whole[:,-n_val:]\n",
    "X_train_whole, y_train_whole, Y_train_whole = X_train_whole[:,:-n_val], y_train_whole[:-n_val], Y_train_whole[:,:-n_val]\n",
    "\n",
    "## normalize with mean and std of train set \n",
    "mean = np.mean(X_train_whole, axis=1)\n",
    "std = np.std(X_train_whole, axis=1)\n",
    "\n",
    "X_train_whole = normalize_data(X_train_whole, mean, std)\n",
    "X_val_small = normalize_data(X_val_small, mean, std)\n",
    "\n",
    "data_best = {\"X_train\": X_train_whole, \"y_train\": y_train_whole, \"Y_train\": Y_train_whole, \"X_val\":X_val_small, \"y_val\": y_val_small, \"Y_val\": Y_val_small}"
   ]
  },
  {
   "source": [
    "# Exercise 2: Compute the gradients for the network parameters\n",
    "Check function step by step"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "importlib.reload(mlp)\n",
    "net = mlp.MLP()\n",
    "bs = 20\n",
    "X, Y = X_train[:, :bs], Y_train[:, :bs]\n",
    "P = net.forward_pass(X)\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2.6225131126746075, 3.2170252533911072)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "importlib.reload(mlp)\n",
    "net = mlp.MLP(lamda=0.01)\n",
    "bs = 20\n",
    "X, Y = X_train[:, :bs], Y_train[:, :bs]\n",
    "c = net.compute_cost(X, Y)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mlp)\n",
    "net = mlp.MLP()\n",
    "bs = 1\n",
    "X, Y = X_train[:, :bs], Y_train[:, :bs]\n",
    "P = net.forward_pass(X)\n",
    "net.compute_gradients(X, Y, P)\n",
    "net.update_parameters()"
   ]
  },
  {
   "source": [
    "## Check the gradients "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mlp)\n",
    "bs, d = 1, 20\n",
    "net = mlp.MLP(dims=[d, 50, 10])\n",
    "X, Y = X_train[:d, :bs], Y_train[:d, :bs]\n",
    "P = net.forward_pass(X)\n",
    "net.compute_gradients(X, Y, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([6.842173318640139e-09, 2.486442854455087e-10],\n",
       " [1.6587616452790045e-10, 5.641024873722951e-11],\n",
       " [4.899544291336755e-12, 4.539913783449534e-12],\n",
       " [5.9559019782023934e-12, 9.097046727024605e-12])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "net.compare_gradients(X, Y, h=1e-5, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [1, 32, 64, 100]\n",
    "lambdas = [0, 0.1, 1]\n",
    "\n",
    "for l in lambdas:\n",
    "    for bs in batch_size:\n",
    "        X, Y = X_train[:d, :bs], Y_train[:d, :bs]\n",
    "        net = mlp.MLP(dims=[d, 50, 10], lamda=l)\n",
    "        P = net.forward_pass(X)\n",
    "        net.compute_gradients(X, Y, P)\n",
    "        rerr_w, rerr_b, aerr_w, aerr_b = net.compare_gradients(X, Y, h=1e-5, eps=1e-6)\n",
    "        print(f'Batch size: {bs} - Lambda {l}->\\n relative_error on gWs {rerr_w} \\n relative_error on gbs {rerr_b} \\n absolute error on gWs {aerr_w} \\n absolute error on gbs {aerr_b}   ')"
   ]
  }
 ]
}