# DD2424 Deep Learning in Data Science course
Author: *Kenza Bouzid*

Solutions for labs for DL course at KTH. Each lab contains implementation of neural networks algorithms as well as notebooks with experiments.

## Lab1 - One Layer Network to classify CIFAR-10

### Mandatory 
- Back Propagation for 1 layer network
- Mini Batch gradient descent to optimize Cross Entropy Loss + L2 Regularization 

### Bonus

- Mini Batch gradient descent to optimize Hinge Loss + L2 Regularization
- Tricks and Avenues to improve the network's performance:
  - Early stopping 
  - Learning Rate Decay
  - Xavier Initialization
  - Data Shuffling and Reordering at the beginning of each epoch 


## Lab2 - 2 Layer Network with Cyclic Learning

### Mandatory 

- Back Propagation for 2 layer network
- Mini Batch gradient descent to optimize Cross Entropy Loss + L2 Regularization with cyclic learning

### Bonus

- Find the learning rate boundaries with LR range Test
- Tricks and Avenues to improve the network's performance:
  - Data Augmentation
  - Ensemble Learning: Majority voting 
  - Augmenting the hidden dimension

## Lab3 - k- Layer Network and Batch Normalization

- 

## Lab4 - RNN

- 
