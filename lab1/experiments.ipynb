{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from six.moves import cPickle\n","import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import importlib\n","import utils as ut\n","import functions as fu\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["K, d, n = 10, 3072, 10000\n","np.random.seed(42)\n","mu, sigma = 0, 0.01 \n","batch_start, batch_end= 0, 20"]},{"source":["# Explore cifar-10 dataset "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["importlib.reload(ut)\n","X, y, _ = ut.loadData('data_batch_1', reshape=True, clipping=False)\n","ut.plotCifar(X, y)"]},{"source":["# Exercise 1: Trianing a multi-linear classifier"],"cell_type":"markdown","metadata":{}},{"source":["## Data Preprocessing and Loading "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["importlib.reload(ut)\n","X_train, y_train, Y_train = ut.loadData('data_batch_1', clipping=True)\n","X_val, y_val, Y_val = ut.loadData('data_batch_2', clipping=True)\n","X_test, y_test, Y_test = ut.loadData('data_batch_3', clipping=True)\n","\n","## normalize with mean and std of train set \n","mean_X = np.mean(X_train, axis=1)\n","std_X = np.std(X_train, axis=1)\n","\n","X_train -= np.outer(mean_X, np.ones(X_train.shape[1]))\n","X_train /= np.outer(std_X, np.ones(X_train.shape[1]))\n","\n","X_val -= np.outer(mean_X, np.ones(X_val.shape[1]))\n","X_val /= np.outer(std_X, np.ones(X_val.shape[1]))\n","\n","X_test -= np.outer(mean_X, np.ones(X_test.shape[1]))\n","X_test /= np.outer(std_X, np.ones(X_test.shape[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Y_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mean_X.shape"]},{"source":["## Initialize parameter"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["W = np.random.normal(mu, sigma, (K,d))\n","b = np.random.normal(mu, sigma, (K,1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["importlib.reload(ut)\n","P = ut.EvaluateClassifier(X_train[:, :100], W, b)\n","print(P.shape)\n","print(np.argmax(P, axis=0))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.sum(np.multiply(Y_train[:,:100], P), axis=0).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["importlib.reload(ut)\n","cost = ut.ComputeCost(X_train[:,:100], Y_train[:, :100], W , b, 1e-2)\n","cost"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["acc = ut.ComputeAccuracy(X_train[:, :100], y_train[:100], W, b)\n","acc"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["importlib.reload(ut)\n","_lambda = 1e-2\n","P = ut.EvaluateClassifier(X_train[:, batch_start:batch_end], W, b)\n","ga_w, ga_b = ut.ComputeGradients(X_train[:, batch_start:batch_end], Y_train[:, batch_start:batch_end], P, W, _lambda)\n","print(ga_w.shape, ga_b.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["importlib.reload(fu)\n","gn_w, gn_b = fu.ComputeGradsNumSlow(X_train[:, batch_start:batch_end], Y_train[:, batch_start:batch_end], W, b, _lambda, 1e-6)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.mean(gn_w - ga_w)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["importlib.reload(ut)\n","np.mean(ut.compare_gradients(ga_w, gn_w, eps=1e-4))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.mean(ut.compare_gradients(ga_b.reshape(-1,1), gn_b.reshape(-1,1), eps=1e-4))"]},{"source":["## MiniBatch Train"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["importlib.reload(ut)\n","\n","W = np.random.normal(mu, sigma, (K,d))\n","b = np.random.normal(mu, sigma, (K,1))\n","GDparams = {\"n_batch\":100, \"n_epochs\":40, \"eta\":1e-3, \"lambda\":0}\n","W, b, train_loss, val_loss, train_acc, val_acc = ut.minibatchGD(X_train, Y_train, y_train,  X_val, Y_val, y_val, GDparams, W, b, verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["importlib.reload(ut)\n","ut.montage(W, GDparams)\n","ut.plot_metric(train_loss, val_loss, GDparams, type=\"loss\")\n","ut.plot_metric(train_acc, val_acc, GDparams, type=\"accuracy\")"]},{"source":["## Experiments"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["importlib.reload(ut)\n","\n","GDparams = [{\"lambda\":0, \"n_epochs\":40, \"n_batch\":100, \"eta\":.1}, {\"lambda\":0, \"n_epochs\":40, \"n_batch\":100, \"eta\":.001}, \n","          {\"lambda\":.1, \"n_epochs\":40, \"n_batch\":100, \"eta\":.001}, {\"lambda\":1, \"n_epochs\":40, \"n_batch\":100, \"eta\":.001}]\n","\n","np.random.seed(42)\n","seeds = np.random.randint(0, 100, 5)\n","stats = {i:{\"val_loss\":[], \"train_loss\":[], \"val_acc\":[], \"train_acc\":[]}for i in range(4)}\n","\n","for i, GDparam in enumerate(GDparams): \n","    for seed in seeds:\n","        np.random.seed(seed)\n","        W = np.random.normal(mu, sigma, (K,d))\n","        b = np.random.normal(mu, sigma, (K,1))\n","        W, b, train_loss, val_loss, train_acc, val_acc = ut.minibatchGD(X_train, Y_train, y_train,  X_val, Y_val, y_val, GDparam, W, b, verbose=False)\n","\n","        stats[i][\"train_loss\"].append(train_loss[-1])\n","        stats[i][\"val_loss\"].append(val_loss[-1])\n","        stats[i][\"train_acc\"].append(train_acc[-1])\n","        stats[i][\"val_acc\"].append(val_acc[-1])\n","    \n","    ut.montage(W, GDparams)\n","    ut.plot_metric(train_loss, val_loss, GDparams, type=\"loss\")\n","    ut.plot_metric(train_acc, val_acc, GDparams, type=\"accuracy\")\n","np.save(\"History/stats.npy\", stats)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(4):\n","    stats[i]['stats_val_acc'] = [round(np.mean(stats[i]['val_acc']),5), round(np.std(stats[i]['val_acc']),5)]\n","    stats[i]['stats_train_acc'] = [round(np.mean(stats[i]['train_acc']),5), round(np.std(stats[i]['train_acc']),5)]\n","    stats[i]['stats_val_loss'] = [round(np.mean(stats[i]['val_loss']),5), round(np.std(stats[i]['val_loss']),5)]\n","    stats[i]['stats_train_loss'] = [round(np.mean(stats[i]['train_loss']),5), round(np.std(stats[i]['train_loss']),5)]\n","\n","    print(f\"Conf {i} -> val_acc: {stats[i]['stats_val_acc']} train_acc: {stats[i]['stats_train_acc']} val_loss: {stats[i]['stats_val_loss']}train_loss: {stats[i]['stats_train_loss']}\")\n","\n","np.save(\"History/stats.npy\", stats)"]},{"source":["# Bonus\n","## Early Stopping "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{"tags":["outputPrepend"]},"outputs":[],"source":["importlib.reload(ut)\n","setting = {\"lambda\":0.1, \"n_epochs\":200, \"n_batch\":100, \"eta\":.01}\n","np.random.seed(42)\n","mu, sigma = 0, 0.01\n","W = np.random.normal(mu, sigma, (K,d))\n","b = np.random.normal(mu, sigma, (K,1))\n","W, b, train_loss, val_loss, train_acc, val_acc = ut.minibatchGD(X_train, Y_train, y_train,  X_val, Y_val, y_val, setting, W, b, verbose=True, patience=5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ut.montage(W, setting)\n","ut.plot_metric(train_loss, val_loss, setting, type=\"loss\")\n","ut.plot_metric(train_acc, val_acc, setting, type=\"accuracy\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_acc[-1]"]},{"source":["## Xavier Initialization"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["importlib.reload(ut)\n","setting = {\"lambda\":.1, \"n_epochs\":40, \"n_batch\":50, \"eta\":.001}\n","\n","np.random.seed(42)\n","seeds = np.random.randint(0, 100, 5)\n","stats_xavier = {\"val_loss\":[], \"train_loss\":[], \"val_acc\":[], \"train_acc\":[]}\n","\n","for seed in seeds:\n","    np.random.seed(seed)\n","    mu, sigma = 0, 1/np.sqrt(d)\n","    W = np.random.normal(mu, sigma, (K,d))\n","    b = np.random.normal(mu, sigma, (K,1))\n","    W, b, train_loss, val_loss, train_acc, val_acc = ut.minibatchGD(X_train, Y_train, y_train,  X_val, Y_val, y_val, setting, W, b, verbose=False)\n","\n","    stats_xavier[\"train_loss\"].append(train_loss[-1])\n","    stats_xavier[\"val_loss\"].append(val_loss[-1])\n","    stats_xavier[\"train_acc\"].append(train_acc[-1])\n","    stats_xavier[\"val_acc\"].append(val_acc[-1])\n","\n","\n","stats_xavier['stats_val_acc'] = [round(np.mean(stats_xavier['val_acc']),5), round(np.std(stats_xavier['val_acc']),5)]\n","stats_xavier['stats_train_acc'] = [round(np.mean(stats_xavier['train_acc']),5), round(np.std(stats_xavier['train_acc']),5)]\n","stats_xavier['stats_val_loss'] = [round(np.mean(stats_xavier['val_loss']),5), round(np.std(stats_xavier['val_loss']),5)]\n","stats_xavier['stats_train_loss'] = [round(np.mean(stats_xavier['train_loss']),5), round(np.std(stats_xavier['train_loss']),5)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","print(f\"Conf -> val_acc: {stats_xavier['stats_val_acc']} train_acc: {stats_xavier['stats_train_acc']} val_loss: {stats_xavier['stats_val_loss']}train_loss: {stats_xavier['stats_train_loss']}\")\n","\n","np.save('History/stats_xavier.npy', stats_xavier)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ut.montage(W, setting)\n","ut.plot_metric(train_loss, val_loss, setting, type=\"loss\")\n","ut.plot_metric(train_acc, val_acc, setting, type=\"accuracy\")"]},{"source":["## Learning rate Annealing "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["importlib.reload(ut)\n","setting = {\"lambda\":.1, \"n_epochs\":100, \"n_batch\":50, \"eta\":.01, \"eta_decay\":0.9 , 'eta_decay_freq':10}\n","mu, sigma = 0, 0.01\n","W = np.random.normal(mu, sigma, (K,d))\n","b = np.random.normal(mu, sigma, (K,1))\n","W, b, train_loss, val_loss, train_acc, val_acc = ut.minibatchGD(X_train, Y_train, y_train,  X_val, Y_val, y_val, setting, W, b, verbose=True, annealing=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ut.montage(W, setting)\n","ut.plot_metric(train_loss, val_loss, setting, type=\"loss\")\n","ut.plot_metric(train_acc, val_acc, setting, type=\"accuracy\")"]},{"source":["## Shuffle the order "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["importlib.reload(ut)\n","setting = {\"lambda\":.1, \"n_epochs\":40, \"n_batch\":50, \"eta\":.001}\n","\n","np.random.seed(42)\n","seeds = np.random.randint(0, 100, 5)\n","stats_shuffle = {\"val_loss\":[], \"train_loss\":[], \"val_acc\":[], \"train_acc\":[]}\n","\n","for seed in seeds:\n","    np.random.seed(seed)\n","    mu, sigma = 0, 0.01 \n","    W = np.random.normal(mu, sigma, (K,d))\n","    b = np.random.normal(mu, sigma, (K,1))\n","    W, b, train_loss, val_loss = ut.minibatchGD(X_train, Y_train, y_train,  X_val, Y_val, y_val, setting, W, b, verbose=True, patience=2, reorder=True)\n","\n","    stats_shuffle[\"train_loss\"].append(train_loss[-1])\n","    stats_shuffle[\"val_loss\"].append(val_loss[-1])\n","    stats_shuffle[\"train_acc\"].append(train_acc[-1])\n","    stats_shuffle[\"val_acc\"].append(val_acc[-1])\n","\n","\n","stats_shuffle['stats_val_acc'] = [round(np.mean(stats_shuffle['val_acc']),5), round(np.std(stats_shuffle['val_acc']),5)]\n","stats_shuffle['stats_train_acc'] = [round(np.mean(stats_shuffle['train_acc']),5), round(np.std(stats_shuffle['train_acc']),5)]\n","stats_shuffle['stats_val_loss'] = [round(np.mean(stats_shuffle['val_loss']),5), round(np.std(stats_shuffle['val_loss']),5)]\n","stats_shuffle['stats_train_loss'] = [round(np.mean(stats_shuffle['train_loss']),5), round(np.std(stats_shuffle['train_loss']),5)]\n","\n","print(f\"Conf {i} -> val_acc: {stats_shuffle['stats_val_acc']} train_acc: {stats_shuffle['stats_train_acc']} val_loss: {stats_shuffle['stats_val_loss']}train_loss: {stats_shuffle['stats_train_loss']}\")\n","\n","np.save('History/shuffle_stats.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ut.montage(W, setting)\n","ut.plot_metric(train_loss, val_loss, setting, type=\"loss\")\n","ut.plot_metric(train_acc, val_acc, setting, type=\"accuracy\")"]},{"source":["## TODOs\n","\n","* Use all the available training data for training (all five batches minus a small\n","subset of the training images for a validation set). Decrease the size of the\n","validation set down to around 1000.\n","* Train for a longer time and use your validation set to make sure you don't\n","overfit or to keep a record of the best model before you begin to overfit.\n","* Play around with decaying the learning rate by a factor around :9 after each epoch.\n","Or you can decay the learning rate by a factor of 10 after every nth epoch.\n","* Shuffle the order of your training examples at the beginning of every epoch."],"cell_type":"markdown","metadata":{}},{"source":["## Bonus SVM"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"orig_nbformat":2,"kernelspec":{"name":"python3612jvsc74a57bd02762fd0ccae7a6827a0de0868563b3d499c815e35640ddddc3d2dc7e9a34dcb9","display_name":"Python 3.6.12 64-bit (conda)"}}}